# train_config.yaml
# Data
train_traces: null  # Required, path to training dataset
do_eval: true
eval_epochs: 0.5
holdout_traces: null  # Required, path to evaluation dataset
max_length: 4096

# Model
student: meta-llama/Llama-3.2-3B
tokenizer: meta-llama/Llama-3.2-3B-Instruct
checkpoint: null
lora: true
lora_r: 128
lora_alpha: 128
lora_dropout: 0.0

seed: 69
wandb: true

# Optimization parameters
lr: 5e-4
weight_decay: 0.1
max_grad_norm: 1.0
warmup: 0.03
lr_scheduler_type: cosine
batch_size: 16
per_device_batch_size: 2
num_epochs: 3

# Output
exp_dir: ${oc.env:EXP_DIR, "debug"}  # Output directory
metadata_dir: ${exp_dir}/metadata  # Metadata directory
model_registry: ${metadata_dir}/model_registry.jsonl  # Path to trace registry
model_dir: ${exp_dir}/models  # model directory
model_name: STUDENT(${student})_seed${seed}_lora${lora_r}_${lora_alpha}_${lora_dropout}_lr${lr}_bs${batch_size}_epochs${num_epochs}
model_path: ${model_dir}/${model_name}
