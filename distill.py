# -*- coding: utf-8 -*-
# ================================================================================
# DISTILL.PY - STUDENT MODEL DISTILLATION FROM TEACHER TRACES
# ================================================================================
# This script performs supervised fine-tuning (SFT) of a student model using
# reasoning traces generated by the teacher model, potentially with antidistillation
# sampling (ADS) applied.
#
# Purpose in the ADS pipeline:
# - Test the effectiveness of antidistillation defense by attempting to distill
#   a student model from "poisoned" teacher traces
# - When lambda=0: baseline distillation with clean traces
# - When lambda>0: distillation with ADS-modified traces (should be less effective)
#
# Key functionality:
# 1. Load training traces (potentially poisoned with ADS) and holdout traces
# 2. Set up student model with optional LoRA for efficient fine-tuning
# 3. Perform supervised fine-tuning on completion-only portions (assistant responses)
# 4. Evaluate student performance to measure distillation effectiveness
# 5. Save the distilled model for subsequent evaluation
#
# Success metrics:
# - High student performance with lambda=0 indicates good baseline distillation
# - Low student performance with lambda>0 indicates effective ADS defense
# ================================================================================

import logging
import os
from io import StringIO
from types import SimpleNamespace

import datasets
import hydra
import torch
import yaml
from accelerate import Accelerator
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf
from peft import LoraConfig, get_peft_model
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import logging as hf_logging
from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer

import wandb
from utils import SYSTEM_PROMPT, init

# ================================================================================
# DISTRIBUTED SETUP AND LOGGING CONFIGURATION
# ================================================================================
accelerator = Accelerator()
log = logging.getLogger(__name__)

# Disable verbose logging for non-main processes to reduce noise
if not accelerator.is_main_process:
    hf_logging.set_verbosity_error()
    hf_logging.disable_progress_bar()
    datasets.disable_progress_bar()
    tqdm = lambda x, *args, **kwargs: x

def log_color(content, title=""):
    """Enhanced logging with colored console output for main process."""
    try:
        console = Console()
        console.print(Panel(content, title=title, border_style="cyan", title_align="left"))

        # Log the message as plain text for log files
        string_io = StringIO()
        plain_console = Console(file=string_io, highlight=False)
        plain_console.print(Panel(content, title=title, border_style="none", title_align="left"))
        log.info("\n" + string_io.getvalue())
    except Exception as e:
        log.error(f"Error logging content: {e}")

# ================================================================================
# MAIN DISTILLATION FUNCTION
# ================================================================================

@hydra.main(config_path=".", config_name="train_config", version_base="1.3")
def main(cfg: DictConfig):
    # ============================================================================
    # CONFIGURATION VALIDATION AND SETUP
    # ============================================================================
    # Validate required configurations for distillation
    assert cfg.train_traces, "Please provide the training traces"
    if cfg.do_eval:
        assert cfg.holdout_traces, "Please provide the holdout traces for evaluation"
    
    # Set default tokenizer to match student model if not specified
    cfg.tokenizer = cfg.tokenizer or cfg.student
    
    # Load trace generation configuration to understand the data characteristics
    with open(cfg.train_traces+".yaml", 'r') as f:
        trace_config = yaml.safe_load(f)
        trace_cfg = SimpleNamespace(**trace_config)

    # Initialize random seeds for reproducibility
    init(os.getenv("USER"), cfg.seed)

    # Display configuration for debugging and reproducibility
    if accelerator.is_main_process:
        content = Syntax(OmegaConf.to_yaml(cfg, resolve=True), 'yaml', theme="monokai")
        log_color(content, title="Model Config")
        content = Syntax(yaml.dump(trace_config), 'yaml', theme="monokai")
        log_color(content, title="Trace Config")

    # Disable Weights & Biases if not needed
    if not cfg.wandb:
        os.environ["WANDB_DISABLED"] = "true"

    # ============================================================================
    # TOKENIZER SETUP
    # ============================================================================
    # Configure tokenizer to match the one used for trace generation
    # This ensures consistent tokenization between teacher traces and student training
    tokenizer = AutoTokenizer.from_pretrained(
        cfg.tokenizer,
        use_fast=True,
        fast_tokenizer=True,
        trust_remote_code=True,
        padding_side="left"  # Left padding for generation tasks
    )
    
    # Model-specific tokenizer configuration (consistent with gentraces.py)
    if "llama" in cfg.tokenizer.lower():
        eot_token_id = 128009
        eos_token_id = 128001
        tokenizer.pad_token_id = 128004
        tokenizer.eos_token_id = eos_token_id
        tokenizer.add_eos_token = False
    elif "qwen" in cfg.tokenizer.lower():
        eos_token = tokenizer.eos_token
        special_tokens = {"pad_token": "[PAD]"}
        tokenizer.add_special_tokens(special_tokens)
    else:
        raise ValueError(f"Unknown tokenizer {cfg.tokenizer}")

    # ============================================================================
    # STUDENT MODEL SETUP
    # ============================================================================
    # Load the student model that will learn from teacher traces
    # This is the "attacking" model attempting to steal teacher capabilities
    student = AutoModelForCausalLM.from_pretrained(
        cfg.student,
        trust_remote_code=True,
        attn_implementation="flash_attention_2",  # Use Flash Attention for efficiency
        torch_dtype=torch.bfloat16,  # Mixed precision for memory efficiency
        use_cache=True,
    )
    student.generation_config.pad_token_id = tokenizer.pad_token_id
    student.generation_config.add_eos_token = False

    # ============================================================================
    # LORA CONFIGURATION (OPTIONAL)
    # ============================================================================
    # Apply LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning
    # This is especially useful for large student models to reduce memory usage
    # In separate hyperparameter experiments, we have noticed that LoRA improves distillation effectiveness even when full finetuning is possible.
    if cfg.lora:
        peft_config = LoraConfig(
            r=cfg.lora_r,                    # Rank of adaptation matrices
            lora_alpha=cfg.lora_alpha,       # Scaling factor for LoRA
            target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],  # Target attention and MLP layers
            lora_dropout=cfg.lora_dropout,   # Dropout for LoRA layers
            bias="none",                     # No bias adaptation
            task_type="CAUSAL_LM",          # Causal language modeling task
        )
        student = get_peft_model(student, peft_config)

    # ============================================================================
    # DATASET PREPROCESSING
    # ============================================================================
    # Preprocess training and evaluation traces on the main process
    if accelerator.is_main_process:
        # Determine suffix length for different tokenizers to remove extra tokens
        suffix_len = -1 if "llama" in cfg.tokenizer.lower() else -2
        
        def preprocess_function(examples):
            """
            Preprocess reasoning traces for supervised fine-tuning.
            
            Converts teacher traces into chat format for student training:
            - Extracts assistant responses from teacher traces
            - Creates chat messages with system prompt, user problem, and assistant response
            - Tokenizes into format suitable for completion-only training
            """
            trace_colname = trace_cfg.trace_colname
            responses = []
            
            # Extract assistant responses from teacher traces
            for response in examples[trace_colname]:
                # Split on assistant marker to get only the response portion
                fixed_response = response.split("<｜Assistant｜>")[1]
                # Replace custom EOS tokens with standard tokenizer EOS
                fixed_response = fixed_response.replace("", tokenizer.eos_token)
                responses.append(fixed_response)
            
            # Create chat format messages for each example
            messages = [[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": problem.strip()},
                {"role": "assistant", "content": response.strip()}]
                for problem, response in zip(examples["problem"], responses)]
            
            # Apply chat template and tokenize
            tokens = tokenizer.apply_chat_template(messages, add_generation_prompt=False)
            tokens = [toks[:suffix_len] for toks in tokens]  # Remove extra tokens
            tok_lengths = [len(toks) for toks in tokens]
            return {"input_ids": tokens, "token_lengths": tok_lengths}

        # Process training traces (potentially poisoned with ADS)
        train_traces = datasets.load_from_disk(cfg.train_traces)
        train_traces = train_traces.map(
            preprocess_function,
            batched=True,
            batch_size=16384,
            num_proc=96,
            remove_columns=list(train_traces.column_names),
            desc="Preprocessing train dataset",
            load_from_cache_file=True
        )
        log_color(tokenizer.decode(train_traces[0]['input_ids']), title="Example Input")
        train_token_length_stats = train_traces.to_pandas()["token_lengths"].describe()
        log_color(str(train_token_length_stats.round(2)), title="Train Trace Token Lengths")
        train_traces = train_traces.remove_columns("token_lengths")
        train_traces.save_to_disk("/tmp/cached_train_traces")

        # Process holdout traces for evaluation (clean, no ADS)
        holdout_traces = datasets.load_from_disk(cfg.holdout_traces)
        holdout_traces = holdout_traces.map(
            preprocess_function,
            batched=True,
            batch_size=16384,
            num_proc=96,
            remove_columns=list(holdout_traces.column_names),
            desc="Preprocessing holdout dataset",
            load_from_cache_file=True
        )
        log_color(str(holdout_traces.to_pandas()["token_lengths"].describe().round(2)), title="Holdout Trace Token Lengths")
        holdout_traces = holdout_traces.remove_columns("token_lengths")
        holdout_traces.save_to_disk("/tmp/cached_holdout_traces")
    
    # Synchronize processes and load cached datasets
    accelerator.wait_for_everyone()
    train_traces = datasets.load_from_disk("/tmp/cached_train_traces")
    holdout_traces = datasets.load_from_disk("/tmp/cached_holdout_traces")

    # ============================================================================
    # DATA COLLATOR SETUP FOR COMPLETION-ONLY TRAINING
    # ============================================================================
    # Set up completion-only training to compute loss only on assistant responses
    # This is crucial for distillation as we want the student to learn the reasoning
    # patterns from the teacher, not just copy the entire input
    if 'llama' in cfg.student.lower():
        response_string = "<|start_header_id|>assistant<|end_header_id|>\n\n"
    elif "qwen" in cfg.student.lower():
        response_string = "<|im_start|>assistant\n"
    else:
        raise ValueError(f"Unknown model {cfg.student}")
    
    collator = DataCollatorForCompletionOnlyLM(
        response_template=tokenizer.encode(response_string, add_special_tokens=False),
        tokenizer=tokenizer,
        mlm=False  # Not using masked language modeling
    )

    # ============================================================================
    # TRAINING CONFIGURATION AND SETUP
    # ============================================================================
    # Configure training parameters and create SFT trainer
    steps_per_epoch = len(train_traces) // cfg.batch_size
    eval_steps = int(steps_per_epoch * cfg.eval_epochs) if cfg.do_eval else 0
    
    trainer = SFTTrainer(
        model=student,
        train_dataset=train_traces,
        eval_dataset=holdout_traces,
        processing_class=tokenizer,
        data_collator=collator,
        args=SFTConfig(
            bf16=student.config.use_bfloat16,  # Use bfloat16 if model supports it
            do_eval=cfg.do_eval,
            max_length=cfg.max_length,
            eval_strategy="steps" if cfg.do_eval else "no",
            eval_steps=eval_steps,
            eval_on_start=True if cfg.do_eval else False,
            gradient_accumulation_steps=cfg.batch_size // cfg.per_device_batch_size // accelerator.num_processes,
            max_grad_norm=cfg.max_grad_norm,   # Gradient clipping for stability
            gradient_checkpointing=False,      # Disabled for speed
            gradient_checkpointing_kwargs={"use_reentrant": False},
            learning_rate=cfg.lr,
            weight_decay=cfg.weight_decay,
            log_level="info",
            logging_steps=10,
            logging_strategy="steps",
            lr_scheduler_type=cfg.lr_scheduler_type,
            optim='adamw_torch_fused',         # Fused AdamW for efficiency
            num_train_epochs=cfg.num_epochs,
            output_dir=cfg.model_path,
            overwrite_output_dir=True,
            per_device_train_batch_size=cfg.per_device_batch_size,
            per_device_eval_batch_size=cfg.per_device_batch_size*2,  # Larger eval batch
            report_to="wandb" if cfg.wandb else None,
            save_strategy="steps",
            save_steps=1000,
            save_total_limit=3,               # Keep only 3 most recent checkpoints
            seed=cfg.seed,
            warmup_ratio=cfg.warmup,
            remove_unused_columns=False,
            label_names=["labels"],
            ddp_find_unused_parameters=False,  # DDP optimization
            save_safetensors=False,            # Use pickle format for compatibility
        ),
    )

    # ============================================================================
    # WANDB LOGGING SETUP (OPTIONAL)
    # ============================================================================
    # Set up experiment tracking if Weights & Biases is enabled
    if accelerator.is_main_process and cfg.wandb:
        wandb_run = wandb.init(
            project="antidistillation",
            name=f"{cfg.exp_dir}/{cfg.model_name}",
            config={**cfg, "trace_config": trace_config},
        )
        
        # Log training trace statistics for analysis
        wandb.log({
            "train_trace_raw_accuracy": trace_cfg.stats["raw_accuracy"],
            "train_trace_af_accuracy": trace_cfg.stats["af_accuracy"],
            "trace_token_length/stats": {k: float(v) for k,v in train_token_length_stats.items()}
        })

        # Save complete configuration with wandb run ID for evaluation scripts
        full_cfg = OmegaConf.to_container(cfg, resolve=True)
        hydra_cfg = HydraConfig.get()
        full_cfg["hydra"] = {
            "run_dir": hydra_cfg.run.dir,
            "job_name": hydra_cfg.job.name,
            "cwd": hydra_cfg.runtime.cwd,
        }
        full_cfg["wandb_run_id"] = wandb_run.id
        yaml_path = cfg.model_path + ".yaml"
        with open(yaml_path, "w") as f:
            OmegaConf.save(full_cfg, f)

    # ============================================================================
    # TRAINING EXECUTION
    # ============================================================================
    # Perform the actual distillation training
    # This is where the student attempts to learn from teacher traces
    # Success here indicates potential vulnerability to distillation attacks
    train_result = trainer.train(resume_from_checkpoint=cfg.checkpoint)

    # ============================================================================
    # EVALUATION (OPTIONAL)
    # ============================================================================
    # Evaluate student performance on holdout data to measure distillation effectiveness
    if cfg.do_eval:
        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(trainer.eval_dataset)
        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)

    # ============================================================================
    # MODEL SAVING AND CLEANUP
    # ============================================================================
    # Save the distilled student model for subsequent evaluation
    if accelerator.is_main_process:
        # Re-enable caching for inference
        trainer.model.config.use_cache = True
        
        # Merge LoRA weights back into base model if LoRA was used
        if cfg.lora:
            trainer.model = trainer.model.merge_and_unload()
        
        torch.cuda.empty_cache()
        
        # Save final model and tokenizer
        final_output_dir = os.path.join(cfg.model_path, "final")
        trainer.save_model(final_output_dir)
        trainer.tokenizer.save_pretrained(final_output_dir)

        # Clean up wandb logging
        if cfg.wandb:
            wandb.finish()

    accelerator.end_training()

if __name__ == "__main__":
    main()